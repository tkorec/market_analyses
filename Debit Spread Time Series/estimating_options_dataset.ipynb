{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomas Korec | +420 739 708 250 | tk.korec@gmail.com | www.korec-finance.cz | https://www.linkedin.com/in/tomas-korec-4419aa214/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "import requests\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType, StringType, ArrayType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# GIT: git push market_analyses options_debit_spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP PYSPARK for faster data processing\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/homebrew/bin/python3.11\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/opt/homebrew/bin/python3.11\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"CallDebitSpread\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.pyspark.python\", \"/opt/homebrew/bin/python3.11\") \n",
    "    .config(\"spark.pyspark.driver.python\", \"/opt/homebrew/bin/python3.11\")  \n",
    "    .getOrCreate()\n",
    "    )\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(spark)\n",
    "\n",
    "print(\"Driver Python:\", spark.conf.get(\"spark.pyspark.driver.python\"))\n",
    "print(\"Worker Python:\", spark.conf.get(\"spark.pyspark.python\"))\n",
    "\n",
    "print(\"PYSPARK_PYTHON:\", os.environ.get(\"PYSPARK_PYTHON\"))\n",
    "print(\"PYSPARK_DRIVER_PYTHON:\", os.environ.get(\"PYSPARK_DRIVER_PYTHON\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL'S RESEARCH OBJECTIVE\n",
    "\n",
    "The idea of this research is based on the assumption that if 95% confidential boundaries produced by time series predicting model define the area an asset's price will develop in, opening a Call Debit Spread with short call strike price being In-The-Money in this area of 95% probability defines the odds for ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"XLV\"\n",
    "\n",
    "\n",
    "data = yf.download(\n",
    "    ticker, # Ticker is chosen based on for what risky asset the analyses is being performed\n",
    "    start=(datetime.today() - timedelta(days=10*365)).strftime(\"%Y-%m-%d\"),\n",
    "    end=datetime.today().strftime(\"%Y-%m-%d\"),\n",
    "    interval=\"1d\"\n",
    ")\n",
    "\n",
    "\n",
    "vix_data = yf.download(\n",
    "    \"^VIX\",\n",
    "    start=(datetime.today() - timedelta(days=10*365)).strftime(\"%Y-%m-%d\"),\n",
    "    end=datetime.today().strftime(\"%Y-%m-%d\"),\n",
    "    interval=\"1d\"\n",
    ")\n",
    "\n",
    "risk_free_rate_data = yf.download(\n",
    "    \"^TNX\",\n",
    "    start=(datetime.today() - timedelta(days=10*365)).strftime(\"%Y-%m-%d\"),\n",
    "    end=datetime.today().strftime(\"%Y-%m-%d\"),\n",
    "    interval=\"1d\"\n",
    ")\n",
    "\n",
    "# Unify the data form and remove name of an index column\n",
    "data = pd.DataFrame({\n",
    "    \"Date\": data.index,\n",
    "    \"Close\": data[\"Close\"][ticker]\n",
    "})\n",
    "\n",
    "data.index.name = None\n",
    "\n",
    "# Add 50 and 100 days Moving Averages to data of risky asset\n",
    "data[\"50_day_MA\"] = data[\"Close\"].rolling(window=50).mean()\n",
    "data[\"100_day_MA\"] = data[\"Close\"].rolling(window=100).mean()\n",
    "\n",
    "# Selected events during which condition on which base the call debit spread would be opened\n",
    "selected_dates = data[\n",
    "    (data[\"50_day_MA\"] > data[\"100_day_MA\"]) &\n",
    "    (data[\"Close\"] > data[\"100_day_MA\"]) &\n",
    "    (data[\"Close\"] < data[\"50_day_MA\"]) &\n",
    "    (data.index <= (datetime.today() - timedelta(days=365)))\n",
    "]\n",
    "\n",
    "# Visualisation of risky asset's time series, moving averages, and call debit spread events\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data[\"Close\"], label=\"Closing Price\")\n",
    "plt.plot(data[\"50_day_MA\"], label=\"50-Day Moving Average\", color=\"orange\", linestyle=\"--\")\n",
    "plt.plot(data[\"100_day_MA\"], label=\"100-Day Moving Average\", color=\"red\", linestyle=\"--\")\n",
    "plt.scatter(selected_dates.index, selected_dates[\"Close\"], color=\"r\", marker=\"o\")\n",
    "plt.title(f\"{ticker} Closing Prices\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price (USD)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPROXIMATING IMPLIED VOLATILITY\n",
    "\n",
    "The value of Implied Volatility (IV, i.e., future volatility expectation) is constantly being determined by market forces and it can be found numericaly via Black-Scholes model. However, since I don't have historical option prices either, I can approximate IV using historical realized volatility.\n",
    "\n",
    "$$\n",
    "\\sigma_{historical} = \\sqrt{\\frac{252}{N} \\sum_{i=1}^{N} (\\ln(\\frac{S_i}{S_{i-1}}))^2}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$N$ = number of trading days in the historical window and 252 days is used for annualization of the volatility $\\newline$\n",
    "$S_i$ = stock price on day i $\\newline$\n",
    "$\\sigma_{historical}$ = historical volatility as a proxy for implied volatility\n",
    "\n",
    "Logarithmic returns measure the percentage change in the stock price between two consecutive periods, e.g., days. Relative change expressed by the fraction is transformed into more symmetric measure via natural logarithm.\n",
    "\n",
    "\n",
    ".............\n",
    "\n",
    "\n",
    "\n",
    "BLACK-SCHOLES MODEL\n",
    "\n",
    "$$\n",
    "C = S_0 N(d_1) - K e^{-rT} N(d_2)\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "d_1 = \\frac{\\ln(S_0/K) + (r + \\sigma^2/2)T}{\\sigma \\sqrt{T}}, \\quad\n",
    "d_2 = d_1 - \\sigma \\sqrt{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_historical_volatility(historical_volatility_data):\n",
    "    sum_squared_returns = historical_volatility_data[\"log_returns_squared\"].sum()\n",
    "    n = historical_volatility_data[\"log_returns\"].count()\n",
    "    sigma_historical = np.sqrt((252 / n) * sum_squared_returns)\n",
    "    return round(sigma_historical, 6)\n",
    "\n",
    "def calculate_call_option_price(s_0, strike, sigma, time_to_exercise, r):\n",
    "    d1 = (math.log(s_0/strike) + (r + 0.5 * sigma**2) * time_to_exercise) / (sigma * np.sqrt(time_to_exercise))\n",
    "    d2 = d1 - sigma * np.sqrt(time_to_exercise)\n",
    "    call_price = s_0 * norm.cdf(d1) - strike * np.exp(-r * time_to_exercise) * norm.cdf(d2)      \n",
    "    return call_price  \n",
    "\n",
    "def set_expiration_date(date):\n",
    "    day_deduction = 0\n",
    "    while data[data.index == (date + relativedelta(years=1, days=day_deduction))].shape[0] == 0:\n",
    "        day_deduction -= 1\n",
    "    expiration_date = (date + relativedelta(years=1, days=day_deduction))\n",
    "    return expiration_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL\n",
    "\n",
    "\n",
    "Improvements decisions â€“ for each tested asset, for prediction's higher accuracy, time series preiction model can be found manually instead of having the ones produced universally by Hyndman and Khandakar algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelARIMA():\n",
    "\n",
    "      def __init__(self) -> None:\n",
    "            pass\n",
    "\n",
    "      def hyndman_khandakar(self, series):\n",
    "            auto_model = auto_arima(series)\n",
    "            #forecast, conf_int = auto_model.predict(n_periods=1, return_conf_int=True, alpha=0.05)\n",
    "            #conf_int = conf_int[0]\n",
    "            order = list(auto_model.order)\n",
    "            seasonal_order = list(auto_model.seasonal_order)\n",
    "            parameters = []\n",
    "            parameters.append(order)\n",
    "            parameters.append(seasonal_order)\n",
    "            return parameters\n",
    "      \n",
    "      def check_stationarity(self, data):\n",
    "            result = adfuller(data)\n",
    "            if (result[1] <= 0.05) & (result[4]['5%'] > result[0]):\n",
    "                  return True\n",
    "            else:\n",
    "                  return False\n",
    "        \n",
    "      def invert_differencing(self, forecast, differencing_count, data):\n",
    "            reverted = forecast\n",
    "            for i in range(1, differencing_count + 1):\n",
    "                  reverted = reverted.cumsum() + data[\"Close\"].iloc[-i]\n",
    "            return reverted\n",
    "\n",
    "      def arima_model(self, series, parameters):\n",
    "            order = tuple(parameters[0])\n",
    "            seasonal_order = tuple(parameters[1])\n",
    "            model = sm.tsa.statespace.SARIMAX(series, order=order, seasonal_order=seasonal_order, trend=\"c\")\n",
    "            res = model.fit(disp=False)\n",
    "            forecast = res.get_prediction(steps=1)\n",
    "            expected = forecast.predicted_mean.values[0]\n",
    "            conf_int = forecast.conf_int(alpha=0.05).values.tolist()[0]\n",
    "            return expected, conf_int\n",
    "\n",
    "      def get_parameters(self, data):\n",
    "            close_p = data[\"Close\"]\n",
    "\n",
    "            is_data_stationary = self.check_stationarity(close_p)\n",
    "            while is_data_stationary == False:\n",
    "                  close_p = close_p.diff()\n",
    "                  close_p = close_p.dropna()\n",
    "                  is_data_stationary = self.check_stationarity(close_p)\n",
    "\n",
    "            parameters = self.hyndman_khandakar(close_p)\n",
    "            return parameters\n",
    "      \n",
    "      def model(self, data, parameters):\n",
    "            data = data.sort_values(by=\"Date\", ascending=True)\n",
    "            close_p = data[\"Close\"]\n",
    "            differencing_count = 0\n",
    "            is_data_stationary = self.check_stationarity(close_p)\n",
    "            while is_data_stationary == False:\n",
    "                  close_p = close_p.diff()\n",
    "                  close_p = close_p.dropna()\n",
    "                  differencing_count += 1\n",
    "                  is_data_stationary = self.check_stationarity(close_p)\n",
    "\n",
    "            forecast, conf_int = self.arima_model(close_p, parameters)\n",
    "\n",
    "            conf_int = np.array(conf_int, ndmin=2)  \n",
    "            forecast = pd.Series(forecast)\n",
    "\n",
    "            forecast = self.invert_differencing(forecast, differencing_count, data)\n",
    "            conf_int[:, 0] = self.invert_differencing(conf_int[:, 0], differencing_count, data)\n",
    "            conf_int[:, 1] = self.invert_differencing(conf_int[:, 1], differencing_count, data)\n",
    "\n",
    "            return forecast, conf_int\n",
    "      \n",
    "\n",
    "# Set up Model object and get parameters of ARMA model\n",
    "model = ModelARIMA()\n",
    "parameters = model.get_parameters(data)\n",
    "parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBIT CALL SPREAD\n",
    "\n",
    "Debit Call Spread is an option strategy when Call option with lower strike price is bought while the Call option on the same asset and same expiration date with higher strike price is written.\n",
    "\n",
    "â€“ Max loss of a Call Debit Spread is given by the premium paid for the spread: $$\\text{Max Loss} = \\text{Premium Paid for the Long Call (lower call)} - \\text{Premium Received for the Short Call (higher strike)} = \\text{Net Premium Paid}$$\n",
    "\n",
    "â€“ Max profit of a Call Debit Spread is given by the difference between the strike prices minus the net premium paid: $$\\text{Max Profit} = \\text{(Short Call strike - Long Call strike)} - \\text{Net Premium Paid}$$\n",
    "\n",
    "â€“ Profit/Loss at expiration under the scenario that asset's price at expiration is equal to or higher than strike price of bought call option and lower than strike price of written call option is given by the deduction of bought option strike price from asset's price at expiration minus Net Premium Paid: $$\\text{P/L under } K_1 <= S_T < K_2 \\text{ scenario } = (S_T - K_1) - \\text{Net Premium Paid}$$\n",
    "\n",
    "Before expiration, beyond options' intrinsic value described above, spread value is influenced by options' extrinsic values â€“ time decay (Theta), implied volatility (Vega), and interest rates (Rho) â€“ as well. The call debit spread value is given by prices of call options it consists of.\n",
    "\n",
    "..........\n",
    "..........\n",
    "\n",
    "PROGRAM FLOW:\n",
    "\n",
    "selected_dates â€“ considering the 1 year expiration, selected dates are a series of trading days starting today minus 1 year\n",
    "\n",
    "Iteration through selected_dates. For each selected date, derive ATM strike price from at that time asset's trading price and build an option chain (set of options with various strike prices) containing options with strike prices by 1 USD on both sides, OTM and ITM, from ATM strike price. The last OTM and ITM options are given by 95% confidential boundaries of prediction produced by ARMA model for asset's prices for the last year before selected date. Price of each call option is calculated via Black-Scholes model and proxy of Implied Volatility. All possible Call Debit Spreads are created.\n",
    "\n",
    "OBJECTIVES:\n",
    "1. For all Call Debit Spreads existing within the 95% confidential boundaries, calculate probability of P/L at the expiration date (in 1 year).\n",
    "2. For all Call Debit Spreads existing within the 95% confidential boundaries, calculate probability of achieving 60% of max profit before\n",
    "   expiration.\n",
    "3. What Call Debit Spreads â€“ with what strikes â€“ have the highest probability of profit.\n",
    "\n",
    "SHORTCOMINGS:\n",
    "\n",
    "Prices of historical options aren't the actual prices as they were at that time. Prices are obtained via Black-Scholes model using historical volatility calculated from asset's prices following after the date for which the proxy of implied volatility is calculated. Even though this\n",
    "makes the expected volatility accurate (actual volatility is used), it's different from the expectation at that time, so the call options are\n",
    "misspriced. Moreover, this way, prices of all options in an option chain for one date are calculated using the same proxy of IV, while in reality, implied volatilities differ for various strike prices.\n",
    "This inaccuracy can be addressed by using actual historical option prices from Alphavantage API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dates = selected_dates.sort_values(by=\"Date\", ascending=False)\n",
    "data = data.sort_values(by=\"Date\", ascending=False)\n",
    "spreads = []\n",
    "\n",
    "for i in range(0, selected_dates.shape[0]):\n",
    "    date = selected_dates[\"Date\"].iloc[i]\n",
    "    \n",
    "    #atm_strike_price = round(data[data[\"Date\"] == date][\"Close\"].iloc[0])\n",
    "    forecast, confidential_boundary = model.model(data[data[\"Date\"] <= date], parameters)\n",
    "    atm_strike_price = round(forecast[0])\n",
    "    print(f\"Price on a given date {data[data['Date'] == date]['Close']}\")\n",
    "    print(f\"Forecast for a given date {forecast}\") # Problem is with sorting\n",
    "    print(f\"Confidential boundaries {confidential_boundary}\")\n",
    "    # this will be given by the confidential boundaries obtained via time series predicting model\n",
    "    #strike_list_lower_boundary = round(atm_strike_price - atm_strike_price * 0.05)\n",
    "    #strike_list_upper_boundary = round(atm_strike_price + atm_strike_price * 0.05)\n",
    "    strike_list_lower_boundary = round(confidential_boundary[0][0])\n",
    "    strike_list_upper_boundary = round(confidential_boundary[0][1])\n",
    "    strike_prices_list = [strike for strike in range(strike_list_lower_boundary, strike_list_upper_boundary + 1)]\n",
    "\n",
    "    # Historical volatility as a proxy of implied volatility for a particular historical date on which we want to\n",
    "    # compute option price is calculated on data since that date to presence\n",
    "    historical_volatility_data = data[data[\"Date\"] >= date].sort_values(by=\"Date\", ascending=True)\n",
    "    historical_volatility_data[\"log_returns\"] = np.log(historical_volatility_data[\"Close\"] / historical_volatility_data[\"Close\"].shift(1))\n",
    "    historical_volatility_data[\"log_returns_squared\"] = historical_volatility_data[\"log_returns\"]**2\n",
    "    sigma_historical = calculate_historical_volatility(historical_volatility_data)\n",
    "    \n",
    "    # Create an empty strike price chain â€“ available options for particular strike prices on a particular date\n",
    "    # At this stage, only strike prices and proxy of implied volatilities are known and expiration date is opened\n",
    "    strike_price_chain = {}\n",
    "    strike_price_chain = {\n",
    "        \"strike\": strike_prices_list,\n",
    "        \"price\": np.full(len(strike_prices_list), None),\n",
    "        \"implied_volatility\": np.full(len(strike_prices_list), sigma_historical)\n",
    "    }\n",
    "\n",
    "    strike_price_chain = pd.DataFrame(strike_price_chain)\n",
    "\n",
    "    # Calculating price of options in a strike price chain\n",
    "    strike_price_chain[\"price\"] = strike_price_chain.apply(\n",
    "        lambda row: calculate_call_option_price(\n",
    "            data.loc[date, \"Close\"],  # Stock price (scalar)\n",
    "            row[\"strike\"],  # Strike price (scalar)\n",
    "            row[\"implied_volatility\"],  # Implied volatility (scalar)\n",
    "            historical_volatility_data.shape[-1] / (252 * 2),  # Time to exercise\n",
    "            risk_free_rate_data.loc[date, \"Close\"] / 100  # Risk-free rate adjusted to ratio of 1 as implied volatility\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # From a strike price chain, set of Call Debit Spread is created and inserted into spreads DataFrame\n",
    "    for i, long_call in strike_price_chain.iterrows():\n",
    "        for j, short_call in strike_price_chain.iloc[i+1:].iterrows():\n",
    "            spread_cost = long_call[\"price\"] - short_call[\"price\"]\n",
    "            max_profit = (short_call[\"strike\"] - long_call[\"strike\"]) - spread_cost # !!!! This doesn't seem to be correct\n",
    "\n",
    "            spreads.append({\n",
    "                \"date\": date,\n",
    "                \"expiration_date\": set_expiration_date(date),\n",
    "                \"long_strike\": long_call[\"strike\"],\n",
    "                \"short_strike\": short_call[\"strike\"],\n",
    "                \"long_price\": long_call[\"price\"],\n",
    "                \"short_price\": short_call[\"price\"],\n",
    "                \"spread_cost\": spread_cost,\n",
    "                \"max_profit\": max_profit,\n",
    "                \"max_loss\": spread_cost,\n",
    "                \"expiration_p/l\": None,\n",
    "                \"%_profit\": None,\n",
    "                \"trading_days\": None\n",
    "            })\n",
    "\n",
    "            \n",
    "spreads = pd.DataFrame(spreads)\n",
    "\n",
    "spreads = spreads[\n",
    "    (spreads[\"long_strike\"] < spreads[\"short_strike\"]) &\n",
    "    (spreads[\"short_strike\"] > data.loc[date, \"Close\"])\n",
    "]\n",
    "\n",
    "spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads = spreads[(spreads[\"max_profit\"] > spreads[\"max_loss\"])]\n",
    "spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"expiration_date\", DateType(), True),\n",
    "    StructField(\"long_strike\", DoubleType(), True),\n",
    "    StructField(\"short_strike\", DoubleType(), True),\n",
    "    StructField(\"long_price\", DoubleType(), True),\n",
    "    StructField(\"short_price\", DoubleType(), True),\n",
    "    StructField(\"spread_cost\", DoubleType(), True),\n",
    "    StructField(\"max_profit\", DoubleType(), True),\n",
    "    StructField(\"max_loss\", DoubleType(), True),\n",
    "    StructField(\"expiration_p/l\", StringType(), True),  # Object type â†’ String\n",
    "    StructField(\"%_profit\", StringType(), True),  # Object type â†’ String\n",
    "    StructField(\"trading_days\", ArrayType(DateType()), True)\n",
    "])\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spreads_sdf = spark.createDataFrame(spreads, schema=schema)\n",
    "\n",
    "# Transforming data Pandas DataFrame to Pyspark DataFrame and creating DataFrame of unique trading days\n",
    "data_spark = spark.createDataFrame(data)\n",
    "data_dates = data_spark.select(\"date\").distinct()\n",
    "\n",
    "# DataFrame of spreads and DataFrame of available trading days both contain variable date, so they needed to be remained not to be\n",
    "# mismatched with each other\n",
    "spreads_sdf = spreads_sdf.withColumnRenamed(\"date\", \"spread_date\")\n",
    "data_dates = data_dates.withColumnRenamed(\"date\", \"trading_date\")\n",
    "\n",
    "\n",
    "valid_trading_dates = spreads_sdf.join(\n",
    "    data_dates,\n",
    "    (col(\"spread_date\") <= col(\"trading_date\")) & (col(\"trading_date\") <= col(\"expiration_date\")),\n",
    "    \"inner\"\n",
    ").groupBy(\n",
    "    \"spread_date\", \n",
    "    \"expiration_date\"\n",
    ").agg(\n",
    "    collect_list(\"trading_date\").alias(\"valid_trading_dates\")\n",
    ")\n",
    "\n",
    "valid_trading_dates = valid_trading_dates.withColumn(\n",
    "    \"valid_trading_dates\", \n",
    "    F.expr(\"array_distinct(valid_trading_dates)\")\n",
    ")\n",
    "\n",
    "# Result is a DataFrame with list of all trading days between days of opening a Call Debit Spread position and expiration date\n",
    "# Trading days are given by data DataFrame obtained from Yahoo Finance \n",
    "valid_trading_dates = valid_trading_dates.toPandas()\n",
    "\n",
    "spark.stop() # Stop Pyspark app since I won't need it further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_data_exp_comb = spreads[[\"date\", \"expiration_date\"]].drop_duplicates() # Unique combination of dates and expiration dates\n",
    "\n",
    "# For each unique combination of date and expiration date, list of trading dates between the given date and expiration date from\n",
    "# valid_trading_dates DataFrame is extracted and is inserted into trading_days column in spreads DataFrame of all rows where\n",
    "# given date and expiration date match\n",
    "for date, exp_date in zip(unq_data_exp_comb[\"date\"], unq_data_exp_comb[\"expiration_date\"]):\n",
    "\n",
    "    valid_trading_dates_list = valid_trading_dates.loc[\n",
    "        (pd.to_datetime(valid_trading_dates[\"spread_date\"]) == date) & (pd.to_datetime(valid_trading_dates[\"expiration_date\"]) == exp_date),\n",
    "        \"valid_trading_dates\"\n",
    "    ].values[0]\n",
    "\n",
    "    matching_spreads = spreads.loc[(spreads[\"date\"] == date) & (spreads[\"expiration_date\"] == exp_date)]\n",
    "    for index in matching_spreads.index:\n",
    "        spreads.at[index, \"trading_days\"] = valid_trading_dates_list\n",
    "\n",
    "spreads[\"trading_days\"] = spreads[\"trading_days\"].apply(sorted)\n",
    "\n",
    "spreads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROFIT/LOSS AT EXPIRATION\n",
    "\n",
    "There are only three scenarions:\n",
    "$$\\text{Scenario 1, when: } S_{\\text{asset's price at expiration}} < K_{\\text{strike of call long}} \\text{, then } \\text{\\small (Premium Paid for Long Call - Premium Received for Short Call)}$$\n",
    "$$\\text{Scenario 2, when: } K_{\\text{strike of long call}} <= S_{\\text{asset's price at expiration}} < K_{\\text{strike of short call}} \\text{, then } (S_{\\text{current asset's price}} - K_{\\text{strike of long call}}) - \\text{\\small Net Premium Paid}$$\n",
    "$$\\text{Scenario 3, when: } K_{\\text{strike of short call}} < S_{\\text{asset's price at expiration}} \\text{, then } (S_{\\text{asset's price at expiration}} - K_{\\text{strike of call long}}) - \\text{\\small Net Premium Paid}$$\n",
    "\n",
    "In the case of the 1st scenario, long call option as well as short call one expired worthless, so the loss on paid premium is realized, while profit on premium of written call is gained. However, as long call was at the time of purchase ITM or closer to ITM than written call, higher\n",
    "premium was paid than received, so the maximum loss given by the difference between paid and received premiums is realized.\n",
    "\n",
    "In the case of second scenario, the premium for a short call is received and long call expired In The Money, so the profit given by difference between asset's actual price and long call strike price can be realized. The profit is given by this difference, i.e., with what discount the asset can be bought. If this profit isn't significant enough to cover the premium of a long call, even after receiving premium for short call, loss is realized.\n",
    "\n",
    "In the case of the 3rd scenario, price of an asset at expiration is higher than strike price of a short call which means that it is also higher than than strike price of a long call. Profit at expiration is therefore given by exercising the long call option and receiving profit between asset's actual price and strike price while loss given by short call expiring In The Money must be covered.\n",
    "\n",
    "PROGRAM FLOW\n",
    "\n",
    "For each nested DataFrame and its key (selected date on which the postion of call debit spread was open), calculate P/L on expiration (for the closest trading day) accoring to the three scenarios presente above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, spread in spreads.iterrows():\n",
    "\n",
    "    # Set the expiration date in one year\n",
    "    expiration_date = spread[\"expiration_date\"]\n",
    "    \n",
    "    # Price of an asset at time of a call debit spread's expiration date\n",
    "    asset_price_at_expiration = data.loc[expiration_date][\"Close\"]\n",
    "    \n",
    "    # Calculating proxy of Implied Volatility at the expiration time via historical volatility\n",
    "    historical_volatility_at_expiration = data[data[\"Date\"] >= expiration_date].sort_values(by=\"Date\", ascending=True)\n",
    "    historical_volatility_at_expiration[\"log_returns\"] = np.log(historical_volatility_at_expiration[\"Close\"]) /\\\n",
    "          historical_volatility_at_expiration[\"Close\"].shift(1)\n",
    "    historical_volatility_at_expiration[\"log_returns_squared\"] = historical_volatility_at_expiration[\"log_returns\"]**2\n",
    "    sigma_at_expiration = calculate_historical_volatility(historical_volatility_at_expiration)\n",
    "    \n",
    "    # Risk-free rate on a call debit spread's expiration date\n",
    "    risk_free_rate_at_expiration = risk_free_rate_data.loc[expiration_date, \"Close\"].iloc[0] / 100\n",
    "\n",
    "    if asset_price_at_expiration < spread[\"long_strike\"]:\n",
    "        spreads.loc[index, \"expiration_p/l\"] = 1\n",
    "    elif spread[\"short_strike\"] < asset_price_at_expiration:\n",
    "        spreads.loc[index, \"expiration_p/l\"] = 0\n",
    "    else:\n",
    "        exp_p_and_l_amount = asset_price_at_expiration - spread[\"long_strike\"] - spread[\"spread_cost\"]\n",
    "        spreads.loc[index, \"expiration_p/l\"] = 1 if exp_p_and_l_amount >= 0 else 0\n",
    "    \n",
    "\n",
    "spreads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROFIT/LOSS DURING THE LIFE OF AN OPTION UNTIL EXPIRATION\n",
    "\n",
    "Profit or loss before option's expiration is given not only by intrinsic value, but also the extrinsic one â€“ time decay (Theta), implied volatility (Vega), and interest rates (Rho) â€“ and the profit or loss realized is given by difference of the spread's current and buying price.\n",
    "\n",
    "PROGRAM FLOW\n",
    "\n",
    "For each dataframe, i.e., each option chain on a selected date and each Call Debit Spread in it, do the following -> for each trading day given by data dataframe between date of opening that Call Debit Spread and its expiration, calculate at-that-time spread's price and throughout its entire life, calculate how many time it reached at least 60% of its max profit.\n",
    "\n",
    "The algorithm is implemented this way -> Iteration throughout trading dates in data dataframe. If date (index of data dataframe) is higher than key (date on which the spreads were to be opened) in spreads_grouped DataFrameGoupBy dataframe but lower than what is considered the expiration date, then for all keys/dates/call debit spreads meeting these conditions, calculate at-that-time price of call debit spreads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl_before_expiration(row):\n",
    "\n",
    "    count_of_sixty_percent_return = 0\n",
    "\n",
    "    trading_days_list = row[\"trading_days\"]\n",
    "    for day in trading_days_list:\n",
    "        price = data.loc[data[\"Date\"] == day, \"Close\"].iloc[0]\n",
    "        \n",
    "        # Historical volatility as a proxy of implied volatility for a particular historical date on which we want to\n",
    "        # compute option price is calculated on data since that date to presence\n",
    "        historical_volatility_data = data[data[\"Date\"] >= day].sort_values(by=\"Date\", ascending=True)\n",
    "        historical_volatility_data[\"log_returns\"] = np.log(historical_volatility_data[\"Close\"] / historical_volatility_data[\"Close\"].shift(1))\n",
    "        historical_volatility_data[\"log_returns_squared\"] = historical_volatility_data[\"log_returns\"]**2\n",
    "        sigma = calculate_historical_volatility(historical_volatility_data)\n",
    "\n",
    "        time_to_exercise = len(trading_days_list[trading_days_list.index(day):]) / (252 * 2)\n",
    "\n",
    "        r = (risk_free_rate_data.loc[day, \"Close\"] / 100).iloc[0]\n",
    "\n",
    "        long_call_price = calculate_call_option_price(price, row[\"long_strike\"], sigma, time_to_exercise, r)\n",
    "        short_call_price = calculate_call_option_price(price, row[\"short_strike\"], sigma, time_to_exercise, r)\n",
    "\n",
    "        call_debit_spread_price = long_call_price - short_call_price\n",
    "\n",
    "        #if ((call_debit_spread_price - row[\"spread_cost\"]) / row[\"max_loss\"]) >= -0.5:\n",
    "        if (row[\"spread_cost\"] - call_debit_spread_price) / row[\"max_loss\"] > 0:\n",
    "            #print((row[\"spread_cost\"] - call_debit_spread_price) / row[\"max_loss\"])\n",
    "            spreads[\"%_profit\"] = -1\n",
    "            break\n",
    "\n",
    "        if ((call_debit_spread_price - row[\"spread_cost\"]) / (row[\"max_profit\"] - row[\"spread_cost\"])) >= 0.4:\n",
    "            count_of_sixty_percent_return += 1\n",
    "\n",
    "        return count_of_sixty_percent_return\n",
    "        \n",
    "        \n",
    "spreads[\"%_profit\"] = spreads.apply(pl_before_expiration, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MONTE CARLO SIMULATION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations_number = 1000\n",
    "spreads_per_simulation = 10\n",
    "\n",
    "profits = []\n",
    "losses = []\n",
    "\n",
    "for _ in range(simulations_number):\n",
    "    sample_spreads = spreads.sample(n=spreads_per_simulation, replace=True)\n",
    "    for index, spread in sample_spreads.iterrows():\n",
    "        profit = 0\n",
    "        loss = 0\n",
    "        if spread[\"%_profit\"] == -1:\n",
    "            loss += spread[\"max_loss\"] * 0.5\n",
    "        if spread[\"%_profit\"] == 1:\n",
    "            profit += spread[\"max_profit\"] * 0.4\n",
    "        if spread[\"expiration_p/l\"] == 1:\n",
    "            profit += spread[\"max_profit\"]\n",
    "    profits.append(profit)\n",
    "    losses.append(loss)\n",
    "\n",
    "print(np.mean(profits))\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads[\n",
    "    (spreads[\"expiration_p/l\"] == 0) &\n",
    "    (spreads[\"%_profit\"] == 0)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT SPREADS (WITH WHAT STRIKES) HAVE THE HIGHEST PROFIT PROBABILITY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"WPVNUZAZ3T8UB41B\"\n",
    "date = selected_dates.index[100].strftime('%Y-%m-%d')\n",
    "\n",
    "hist_data = pd.DataFrame(columns=[\"contractID\", \"symbol\", \"expiration\", \"strike\", \"type\", \"last\", \"mark\",\n",
    "       \"bid\", \"bid_size\", \"ask\", \"ask_size\", \"volume\", \"open_interest\", \"date\",\n",
    "       \"implied_volatility\", \"delta\", \"gamma\", \"theta\", \"vega\", \"rho\"])\n",
    "\n",
    "url = f\"https://www.alphavantage.co/query?function=HISTORICAL_OPTIONS&symbol={ticker}&date={date}&apikey={API_KEY}\"\n",
    "response = requests.get(url)\n",
    "hist_opt_data = response.json()\n",
    "hist_opt_data = pd.DataFrame(hist_opt_data[\"data\"])\n",
    "hist_data = pd.concat([hist_data, hist_opt_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUTURE RESEARCH PROPOSITIONS\n",
    "\n",
    "1/ Probability of profits and losses were given by simple condition â€“ describe the condition here â€“ without any advanced conditions specification\n",
    "or looking for the position entry points with the highest returns in percents of the max profit or with the highest number of days during which the profit was 60% of max profit and higher. Further research can be focused on what are the other circumstanes that made some call debit spreads more promising than others. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
